{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5292cc3-4f7b-42e1-b57e-24f76ede4d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lydiaavin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/lydiaavin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports & Setup\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "import PyPDF2 #read pdfs\n",
    "#text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from sentence_transformers import SentenceTransformer # embedding model\n",
    "import faiss # vector search - facebook ai similarity search\n",
    "from transformers import pipeline\n",
    "\n",
    "# sentence tokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" #small, fast\n",
    "embedder = SentenceTransformer(EMBED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cb025e-5b77-44e8-a0e6-3e4ff381aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF as Text - just extracts raw text from pdf\n",
    "def load_text_from_pdf(pdf_path: str) -> str:\n",
    "    text_chunks = []\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for p in range(len(reader.pages)):\n",
    "            page_text = reader.pages[p].extract_text() or \"\"\n",
    "            text_chunks.append(page_text)\n",
    "    return \"\\n\".join(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107cfeb0-366c-496f-b2fd-0bff893101ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunk Text into Passages\n",
    "def chunk_text(text: str, chunk_size_words: int = 300, overlap_words: int = 50) -> List[str]:\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_words = []\n",
    "\n",
    "    for s in sentences:\n",
    "        words = s.split() #sentence to words\n",
    "        if len(current_words) + len(words) <= chunk_size_words:\n",
    "            current_words.extend(words)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_words)) #overlap to avoid missing context\n",
    "            overlap = current_words[-overlap_words:] if overlap_words > 0 else []\n",
    "            current_words = overlap + words\n",
    "\n",
    "    if current_words:\n",
    "        chunks.append(\" \".join(current_words))\n",
    "\n",
    "    return [c.strip() for c in chunks if c.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18defd-f264-4b65-a847-dbfae9a806aa",
   "metadata": {},
   "source": [
    "# Why chunking?\n",
    "LLMs and QA models can't handle very long texts at once.\n",
    "So, we split (chunk) the big PDF text into smaller passages\n",
    "of ~300 words each, with a little overlap to preserve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76387b5c-7107-4dfd-9bff-79d12e0767c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build FAISS Index with Embeddings - nemerical representation of text\n",
    "def build_faiss_index(chunks: List[str]) -> Tuple[faiss.IndexFlatL2, np.ndarray]:\n",
    "    embeds = embedder.encode(chunks, convert_to_numpy=True, show_progress_bar=True) #generates embeddings\n",
    "    dim = embeds.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim) #initialize index\n",
    "    index.add(embeds.astype('float32'))\n",
    "    return index, embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f42834-f695-403e-a1e2-91221d729469",
   "metadata": {},
   "source": [
    "To answer questions, we first need to *retrieve* the most relevant chunks.\n",
    "For retrieval, we:\n",
    "   1. Convert each chunk into a vector (embedding) using SentenceTransformer.\n",
    "   2. Store these vectors inside a FAISS index for fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50c5285c-9dbb-41d3-b91e-e2c094aa4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval & Context Assembly\n",
    "def retrieve_top_k(question: str, chunks: List[str], index: faiss.IndexFlatL2, k: int = 4) -> List[Tuple[int, float]]:\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True).astype('float32') #question to embedding\n",
    "    D, I = index.search(q_emb, k)  # d-similarity score, i-matching indexes\n",
    "    results = []\n",
    "    for idx, dist in zip(I[0], D[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        results.append((int(idx), float(dist)))\n",
    "    return results\n",
    "\n",
    "def assemble_context(retrieved: List[Tuple[int, float]], chunks: List[str], max_concat_chars: int = 1000) -> str:\n",
    "    pieces = []\n",
    "    total_len = 0\n",
    "    for idx, _ in retrieved:\n",
    "        part = chunks[idx].strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        if total_len + len(part) > max_concat_chars and pieces:\n",
    "            break\n",
    "        pieces.append(part)\n",
    "        total_len += len(part)\n",
    "    return \"\\n\\n\".join(pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c28c5-fb04-460d-9b48-a9205ebb9643",
   "metadata": {},
   "source": [
    "   1. Encode the user question into an embedding.\n",
    "   2. Search the FAISS index for the most similar chunks (semantic search).\n",
    "   3. Combine those chunks into a \"context\" to feed into a QA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28c1e9ff-bddb-4e49-9884-21c9be8f479a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "#QA Pipeline\n",
    "qa_model_name = \"deepset/roberta-base-squad2\"  #for extractive question answering\n",
    "qa = pipeline(\"question-answering\", model=qa_model_name, tokenizer=qa_model_name)\n",
    "\n",
    "def answer_question(question, chunks, index, top_k=3):\n",
    "    retrieved = retrieve_top_k(question, chunks, index, k=top_k) #retrieve top k relevant chunks\n",
    "    if not retrieved:\n",
    "        return {\n",
    "            \"answer\": \"No relevant text found.\",\n",
    "            \"score\": 0.0,\n",
    "            \"retrieved\": [],\n",
    "            \"context\": \"\"   \n",
    "        }\n",
    "    \n",
    "    context = assemble_context(retrieved, chunks).strip()\n",
    "    if not context:\n",
    "        return {\n",
    "            \"answer\": \"No context available.\",\n",
    "            \"score\": 0.0,\n",
    "            \"retrieved\": retrieved,\n",
    "            \"context\": \"\"  \n",
    "        }\n",
    "\n",
    "    print(\"Retrieved indices:\", [i for i, _ in retrieved])\n",
    "    print(\"Context length:\", len(context))\n",
    "    \n",
    "    result = qa(question=question, context=context)\n",
    "    return {\n",
    "        \"answer\": result.get(\"answer\"),\n",
    "        \"score\": float(result.get(\"score\", 0.0)),\n",
    "        \"retrieved\": retrieved,\n",
    "        \"context\": context   \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8430f8f4-c629-4906-8e35-97104ed84d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6665bcea8d4e6f934634d5e5d3a4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF length (chars): 140515\n",
      "Number of chunks: 72\n",
      "Index type: <class 'faiss.swigfaiss_avx2.IndexFlatL2'>\n",
      "Embeddings shape: (72, 384)\n",
      "Retrieved indices: [0, 8, 19, 7]\n",
      "Context length: 1835\n",
      "\n",
      "Q: What is machine learning?\n",
      "A: a field of study in artificial intelligence\n",
      "Score: 0.3026069402694702\n",
      "Retrieved indices: [0, 8, 19, 7]\n",
      "Context preview: Machine learning Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions . [1] Within a subdiscipline in machin ...\n",
      "Retrieved indices: [33, 28, 34, 9]\n",
      "Context length: 1837\n",
      "\n",
      "Q: What is overfitting in machine learning?\n",
      "A: Settling on a bad, overly complex theory\n",
      "Score: 0.07107354700565338\n",
      "Retrieved indices: [33, 28, 34, 9]\n",
      "Context preview: or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. [132] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a s ...\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"../data/Machine_learning.pdf\" \n",
    "text = load_text_from_pdf(pdf_path)\n",
    "\n",
    "chunks = chunk_text(text, chunk_size_words=300, overlap_words=50)\n",
    "\n",
    "index, embeddings = build_faiss_index(chunks)\n",
    "\n",
    "print(\"PDF length (chars):\", len(text))\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"Index type:\", type(index))\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"What is machine learning?\",\n",
    "    \"What is overfitting in machine learning?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    r = answer_question(q, chunks, index, top_k=4)\n",
    "    print(\"\\nQ:\", q)\n",
    "    print(\"A:\", r['answer'])\n",
    "    print(\"Score:\", r['score'])\n",
    "    print(\"Retrieved indices:\", [idx for idx, _ in r['retrieved']])\n",
    "    print(\"Context preview:\", r['context'][:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a70e94-3a78-4b43-aecb-ecfc9e1cbb69",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "this demonstrates a **simple Retrieval-Augmented Generation (RAG) QA pipeline** using local embeddings and HuggingFace models:\n",
    "\n",
    "1. **PDF ingestion** → Load PDF into raw text.\n",
    "2. **Chunking** → Split text into smaller passages (~300 words) with overlap to retain context.\n",
    "3. **Embedding + FAISS** → Convert chunks into embeddings and store in a FAISS index for fast similarity search.\n",
    "4. **Question answering** → Given a question, retrieve top-k chunks, assemble context, and pass it to a HuggingFace QA model.\n",
    "5. **Answer + context** → Return the extracted answer, confidence score, retrieved chunk indices, and context snippet.\n",
    "\n",
    "---\n",
    "\n",
    "## Learnings\n",
    "\n",
    "- **Chunking matters:** Smaller chunks with slight overlap preserve context for better answers.\n",
    "- **Embeddings:** SentenceTransformers provides meaningful numerical vectors to compare semantic similarity.\n",
    "- **FAISS Index:** Fast nearest-neighbor search allows retrieval of relevant chunks quickly.\n",
    "- **Context assembly:** Concatenating top-k chunks ensures the QA model has sufficient information.\n",
    "- **QA model:** Extractive QA models (like `roberta-base-squad2`) find answer spans in context; outputs are more accurate if the context is high-quality.\n",
    "\n",
    "---\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Load PDF] --> B[Chunk Text]\n",
    "    B --> C[Embed Chunks with SentenceTransformer]\n",
    "    C --> D[Build FAISS Index]\n",
    "    E[User Question] --> F[Embed Question]\n",
    "    F --> G[Retrieve Top-K Chunks from FAISS]\n",
    "    G --> H[Assemble Context]\n",
    "    H --> I[Pass to QA Model]\n",
    "    I --> J[Return Answer + Score + Context]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
