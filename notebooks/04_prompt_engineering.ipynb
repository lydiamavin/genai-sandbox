{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "328bf4cf-f8d4-4ad7-b6fe-10a91422c2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Hello\n",
      "French: Bonjour\n",
      "\n",
      "English: Hi, my name is Thomas\n",
      "French: Bonjour, mon nom est Thomas\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#direct model translation - no prompting\n",
    "from transformers import pipeline\n",
    "\n",
    "# English → French translation pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\") #t5 handles translation directly\n",
    "\n",
    "sentences = [\n",
    "    \"Hello\",\n",
    "    \"Hi, my name is Thomas\"\n",
    "]\n",
    "\n",
    "for s in sentences:\n",
    "    translation = translator(s)\n",
    "    print(f\"English: {s}\")\n",
    "    print(f\"French: {translation[0]['translation_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ca1a8f8-3e32-4b07-b12b-73095a900022",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### GPT-2 Few-Shot Prompting ###\n",
      "\n",
      "\n",
      "Translate English to French:\n",
      "\n",
      "English: Hello, how are you?\n",
      "French: Bonjour, comment ça va ?\n",
      "\n",
      "English: Good morning\n",
      "French: Bonjour\n",
      "\n",
      "English: My name is Thomas\n",
      "French: Je m'appelle Thomas\n",
      "\n",
      "English: Hello\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n",
      "French:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use GPT-style instruction model (GPT-2 or GPT-J)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "few_shot_prompt = \"\"\"\n",
    "Translate English to French:\n",
    "\n",
    "English: Hello, how are you?\n",
    "French: Bonjour, comment ça va ?\n",
    "\n",
    "English: Good morning\n",
    "French: Bonjour\n",
    "\n",
    "English: My name is Thomas\n",
    "French: Je m'appelle Thomas\n",
    "\n",
    "English: Hello\n",
    "French:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=200)\n",
    "print(\"### GPT-2 Few-Shot Prompting ###\\n\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e586ba7-9585-4495-b7eb-2bc537ce174e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solve this math problem step by step: 20 * 4 = 2.5 * (2.5 + 1.5) * (2.5 + 1.5) * (2.5 + 1.5) * (2.5 + 1.5) * (2.5 + 1.5) * (2.5 + 1.5) * (2.5 + 1.5) * (2.5 + 1.5) * (2.\n"
     ]
    }
   ],
   "source": [
    "# Chain of Thought Prompting\n",
    "cot_prompt = \"Solve this math problem step by step: 20 * 4 =\"\n",
    "\n",
    "inputs = tokenizer(cot_prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4db007-0aee-479b-92f3-781d200a0fea",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "\n",
    "## Few-Shot Prompting:\n",
    "1. GPT-2 is not instruction-tuned; may repeat or give wrong translations.  \n",
    "2. Few-shot examples guide output but results are not perfect.  \n",
    "3. Use T5, mBART, or GPT-3/3.5 for accurate translations.\n",
    "\n",
    "## Chain-of-Thought:\n",
    "1. Encourages step-by-step reasoning, but GPT-2 may be inaccurate.  \n",
    "2. Instruction-tuned LLMs give more reliable CoT outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
